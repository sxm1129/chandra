<div>
<h2>Figure 1: The Transformer - model architecture</h2>
<img alt="Diagram of the Transformer model architecture. The model consists of an Encoder stack (left) and a Decoder stack (right), both composed of N identical layers. The Encoder takes Inputs, which are added to Input Embedding and Positional Encoding. Each layer in the Encoder stack contains a Multi-Head Attention block followed by a Feed Forward block, both preceded by Add &amp; Norm. The Decoder takes Outputs (shifted right), which are added to Output Embedding and Positional Encoding. Each layer in the Decoder stack contains a Masked Multi-Head Attention block, followed by a Multi-Head Attention block, and then a Feed Forward block, all preceded by Add &amp; Norm. The Decoder output is passed through a Linear layer and a Softmax layer to produce Output Probabilities."/>
<p>Diagram of the Transformer model architecture. The model consists of an Encoder stack (left) and a Decoder stack (right), both composed of N identical layers. The Encoder takes Inputs, which are added to Input Embedding and Positional Encoding. Each layer in the Encoder stack contains a Multi-Head Attention block followed by a Feed Forward block, both preceded by Add &amp; Norm. The Decoder takes Outputs (shifted right), which are added to Output Embedding and Positional Encoding. Each layer in the Decoder stack contains a Masked Multi-Head Attention block, followed by a Multi-Head Attention block, and then a Feed Forward block, all preceded by Add &amp; Norm. The Decoder output is passed through a Linear layer and a Softmax layer to produce Output Probabilities.</p>
<h3>3.1 Encoder and Decoder Stacks</h3>
<p><b>Encoder:</b> The encoder is composed of a stack of <math>N = 6</math> identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [1] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is <math>\text{LayerNorm}(x + \text{Sublayer}(x))</math>, where <math>\text{Sublayer}(x)</math> is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <math>d_{\text{model}} = 512</math>.</p>
<p><b>Decoder:</b> The decoder is also composed of a stack of <math>N = 6</math> identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <math>i</math> can depend only on the known outputs at positions less than <math>i</math>.</p>
<h3>3.2 Attention</h3>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum</p>
</div>